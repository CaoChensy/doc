#! https://zhuanlan.zhihu.com/p/431957708

![Image](https://pic4.zhimg.com/80/v2-5db1a82996ec388725185ae900a58008.jpg)

# 【PySpark】02 行与列的操作

---
> 更新日期：2021-11-11
> [全部PySpark相关文章](https://zhuanlan.zhihu.com/p/431959767)
---

## 1. 创建DataFrame

> Add a row

```python
columns = ['id', 'dogs', 'cats']
vals = [(1, 2, 0), (2, 0, 1)]

df = spark.createDataFrame(vals, columns)
newRow = spark.createDataFrame([(4,5,7)], columns)

appended = df.union(newRow)
appended.show()
```


> Types Schema

```python
from pyspark.sql.types import *
pdf3 = pd.read_csv('Repayment.csv')
# create schema for your dataframe
schema = StructType([
    StructField("Customer", StringType(), True),
    StructField("Month", DateType(), True),
    StructField("Amount", IntegerType(), True)])
```

## 2. 选择行、列

## 去重、替换、抽样


## 2. 删除行、列

> select data

Python has no && operator. 
It has and and & where the latter one is the 
correct choice to create boolean expressions 
on Column (| for a logical disjunction and ~ 
for logical negation.

```python
df.where(df.foo > 0 & df.bar < 0))
df.where(df.foo > 0 | df.bar < 0))
```

```python
dataDF = spark.createDataFrame([(66, "a", "4"), 
                                (67, "a", "0"), 
                                (70, "b", "4"), 
                                (71, "d", "4")],
                                ("id", "code", "amt"))
dataDF.withColumn("new_column",
       when((col("code") == "a") | (col("code") == "d"), "A")
      .when((col("code") == "b") & (col("amt") == "4"), "B")
      .otherwise("A1")).show()
```

**待补充**

### 将列转为list
```python
df.select('Name').rdd.map(lambda x: x[0]).collect()
```

> 累加`cumsum`

```python
from pyspark.sql.functions import expr
df.withColumn('cumsum', 
    expr('sum(value) over (order by order_col)')).show()
```

```python
df.withColumn('cumsum', 
    expr('sum(value) over (partition by part_col order by order_col)')).show()
```

```python
from pyspark.sql.functions import sum
from pyspark.sql import Window

df.withColumn('cumsum', 
    sum('value').over(Window.orderBy('order_col'))
).show()

+--------+---------+-----+------+
|part_col|order_col|value|cumsum|
+--------+---------+-----+------+
|       0|        1|    1|     1|
|       0|        2|    0|     1|
|       0|        3|    2|     3|
|       1|        4|    1|     4|
|       1|        5|    2|     6|
+--------+---------+-----+------+

df.withColumn('cumsum', 
    sum('value').over(Window.partitionBy('part_col').orderBy('order_col'))
).show()

+--------+---------+-----+------+
|part_col|order_col|value|cumsum|
+--------+---------+-----+------+
|       0|        1|    1|     1|
|       0|        2|    0|     1|
|       0|        3|    2|     3|
|       1|        4|    1|     1|
|       1|        5|    2|     3|
+--------+---------+-----+------+
```

> window functions

**countDistinct**

```python
df = df.withColumn('distinct_color_count_over_the_last_week', F.size(F.collect_set("color").over(w)))

F.approx_count_distinct("color").over(w)
```

### 取样

> sample

用于从DataFrame中进行采样的方法，
withReplacement关键字参数用于指定是否采用有放回的采样，true为有放回采用，false为无放回的采样，
fraction指定采样的比例，
seed采样种子，相同的种子对应的采样总是相同的，用于场景的复现。

```python
df.sample(withReplacement=True, fraction=0.6, seed=3)
```

> sampleBy(col, fractions, seed=None)

按照指定的col列根据fractions指定的比例进行分层抽样，seed是随机种子，用于场景的复现

```python
df.sampleBy('Genre',{'Male':0.1,'Female':0.15}).groupBy('Genre').count()
```

### selectExpr(*expr)

这个方法是select方法的一个变体，他可以接收一个SQL表达式， 返回新的DataFrame

```python
df.selectExpr("id","name","grade1 * 2 as 2grade1","abs(grade2) as absg2").show()
```
