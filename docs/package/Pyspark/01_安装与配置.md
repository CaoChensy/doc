#! https://zhuanlan.zhihu.com/p/362807220

![Image](https://pic4.zhimg.com/80/v2-5db1a82996ec388725185ae900a58008.jpg)

# 【PySpark】01 安装与配置

---
> 更新日期：2021-11-11
> [全部PySpark相关文章](https://zhuanlan.zhihu.com/p/431959767)
---

## 1. JAVA 安装与环境配置

1. `Java`下载地址：[JAVA Download](https://www.oracle.com/java/technologies/javase-downloads.html)

2. 配置环境变量
    1. 新建系统变量
        1. JAVA_HOME: path\jdkwersion
        2. classpath: %JAVA_HOME%\lib;%JAVA_HOME%\lib\tools.jar
    3. 新增系统Path变量
        4. Path: %JAVA_HOME%\bin
    5. 检查`Java`是否完成安装并完成配置：CMD java -version

> PySpark 安装

1. 安装pyspark: 

```bash
pip install pyspark
```

2. CMD pyspark

> 配置Jupyter notebook启动PySpark的用户变量

**升级Jupyter Notebook**
```bash
pip install --upgrade jupyter notebook
```

```bash
PYSPARK_PYTHON: python
PYSPARK_DRIVER_PYTHON：ipython
PYSPARK_DRIVER_PYTHON_OPTS：notebook
```

> 在Mac OSX上配置PySpark

- [下载](http://spark.apache.org/downloads.html)
- 解压到当前目录下的命令：

```bash
tar -zxvf spark-1.6.1-bin-hadoop2.6.tgz
```

- 把解压出来的文件移动到目标目录/Applications/的命令：

```bash
mv spark-1.6.1-bin-hadoop2.6/ /Applications/spark-1.6.1
```

- 打开环境变量配置文件

```bash
nano ~/.bash_profile
```

- 在最后面添加

```bash
PATH="/Applications/spark-1.6.1/bin:${PATH}"
export PATH
```

- 查看环境变量：

```bash
echo $echo
```

```bash
yarn application -list
yarn application -kill <jobid>
```

> GraphFrames

1. 安装graphframes

```bash
pip3 install graphframes
```

2. 下载jar包，根据spark版本下载对应的jia包

[下载 graphframes .jar](https://spark-packages.org/package/graphframes/graphframes)

3. copy jar包到Python site-packages/pyspark/jars 中

```bash
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/jars
```

4. 使用参数启动pyspark，以便下载所有`graphframe`的`jars`依赖项

```bash
pyspark --packages graphframes:graphframes:0.8.0-spark3.0-s_2.12 --jars graphframes-0.8.0-spark3.0-s_2.12.jar
```

终端显示如下：Ivy Default Cache set to: /root/.ivy2/cache
将路径中的jar包复制到cp /Users/qudian/.ivy2/jars/* .

5.第二次启动pyspark

```bash
pyspark --packages graphframes:graphframes:0.8.0-spark3.0-s_2.12 --jars graphframes-0.8.0-spark3.0-s_2.12.jar
```

6.在jupyter中使用需要添加路径

```bash
vi .zshrc
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS=notebook
```

7.启动jupyter，测试

```bash
pyspark --packages graphframes:graphframes:0.8.0-spark3.0-s_2.12
```

```python
localVertices = [(1,"A"), (2,"B"), (3, "C")]
localEdges = [(1,2,"love"), (2,1,"hate"), (2,3,"follow")]
v = sqlContext.createDataFrame(localVertices, ["id", "name"])
e = sqlContext.createDataFrame(localEdges, ["src", "dst", "action"])
g = GraphFrame(v, e)
```
---
**参考文章：**

---

**待补充**
