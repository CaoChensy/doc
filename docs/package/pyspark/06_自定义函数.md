#! https://zhuanlan.zhihu.com/p/467884247
# PySpark 自定义函数 UDF

### UDF

> UDF row-at-a-time Spark 1.3

```python
import numpy as np
import pandas as pd

pdf = pd.DataFrame(np.random.rand(1000, 3))
pdf = pdf.rename(columns={0: "one", 1: "two", 2: "three"})
pdf["id"] = np.random.randint(0, 50, size=len(pdf))
sdf = spark.createDataFrame(pdf)

from pyspark.sql.types import DoubleType

def plus_one(a):
    return a + 1
    
plus_one_udf = udf(plus_one, returnType=DoubleType())
sdf = sdf.withColumn("one_processed", plus_one_udf(sdf["one"]))
sdf.show()
```

> 修饰自定义函数

```python
@udf(returnType=DoubleType())
def plus_one(a):
    return a + 1

sdf = sdf.withColumn("two_processed", plus_one(sdf["two"]))
sdf.show()
```

> Scalar Pandas UDFs

```python
from pyspark.sql.functions import pandas_udf, PandasUDFType

#使用 pandas_udf 定义一个 Pandas UDF
@pandas_udf('double', PandasUDFType.SCALAR)
#输入/输出都是 double 类型的 pandas.Series
def pandas_plus_one(a):
    return a + 1

sdf.withColumn("one_processed", pandas_plus_one(sdf["one"]))
```

scala pandas UDF定义函数的特点 ：
- 输入、输出数据类型 -> pandas.Series
- 输出数据大小 -> 和输入大小一样
- 函数装饰器中的返回类型 -> 一个 DataType，用于指定返回的 pandas.Series 的类型

> Subtract Mean

下例显示了使用 grouped map Pandas UDFs从组中的three列每个值减去平均值。
每个输入到自定义函数的 pandas.DataFrame 具有相同的 “id” 值。这个用户定
义函数的输入和输出模式是相同的，所以我们将“df.schema” 传递给装饰器 
pandas_udf 来指定模式。

```python
@pandas_udf(sdf.schema, PandasUDFType.GROUPED_MAP)
#Input/output are both a pandas.DataFrame
def subtract_mean(df):
    return df.assign(new_col=df.three - df.three.mean())

sdf.groupby("id").apply(subtract_mean)
```

> 多个字段UDF

```python
def myFunc(data_list):
    for val in data_list:
        b, c = data.split(',')
        # do something

    return whatever

myUdf = udf(myFunc, StringType())

df.withColumn('data', concat_ws(',', col('B'), col('C')))
  .groupBy('A').agg(collect_list('data').alias('data'))
  .withColumn('data', myUdf('data'))
```

```python
def func(data_col1,data_col2):
    xxxx   #进行处理
    return str_out
func_udf = udf( func, StringType())

# 以下两者读取 col 列数据的方式不同 都可以用效果一样
df_out = df.withColumn("new_col", func_udf(df["col1"]，df["col2"]))
df_out = df.withColumn("new_col", func_udf(df.col1，df.col2))
```

```python
df.groupBy('col').agg(MyUDF(F.collect_list(F.when(cond, res).otherwise(res))))
```