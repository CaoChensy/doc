### groupby 条件聚合

```python
from pyspark.sql import Row, functions as F

col_names = ["name", "score"]
value = [
    ("Red", 100.0),
    ("Origen", 80.0),
    ("Yellow", 55.0),
    ("Green", 90.0),
    ("Cyan", 85.0),
    ("Blue", 50.0),
    ("Purple", 70.0),
]
df = spark.createDataFrame(value, col_names)

df.groupBy(
    (F.when(F.col("score")>=60,"pass").otherwise("flunk")).alias("result")
).agg(
    F.count('*').alias("cnt")
).show()
```

```python
col_names = ["name", "date", "score"]
value = [
    ("Ali", "2020-01-01", 10.0),
    ("Ali", "2020-01-02", 15.0),
    ("Ali", "2020-01-03", 20.0),
    ("Ali", "2020-01-04", 25.0),
    ("Ali", "2020-01-05", 30.0),
    ("Bob", "2020-01-01", 15.0),
    ("Bob", "2020-01-02", 20.0),
    ("Bob", "2020-01-03", 30.0)
]
df = spark.createDataFrame(value, col_names)
df.show()

# 我们对每个name按照获得的score排序，代码及结果如下：

from pyspark.sql import Row
from pyspark.sql.window import Window
from pyspark.sql.functions import mean, col
from pyspark.sql import functions as F

win1 = Window.partitionBy("name").orderBy("score")
df_rank = df.withColumn("rank",F.rank().over(win1))
df_rank.show()

# 现在的需求是获取分别获取每个对象的最低分数以及其他分数之和，
# 这种情况下我们需要先对name进行分组，然后在聚合的时候分两种条件对分数求和：
# 最低分和其他分。代码如下：

df_sum = df_rank.groupby("name").agg(
    F.sum(F.when(col("rank")==1,col("score"))).alias("first"), 
    F.sum(F.when(col("rank")!=1,col("score"))).alias("common"))
df_sum.show()
```

```python
win1 = Window.partitionBy("name").orderBy("score")
df_rank = df.withColumn("rank",F.rank().over(win1))
df_rank = df_rank.withColumn("flag",F.when(F.col("rank")==1, 1).otherwise(0))
df_rank.show()

df_flag = df_rank.groupBy("name").agg(
    F.sum(F.col("score")*F.col("flag")).alias("first"),
    F.sum(F.col("score")).alias("all"),
)
df_flag.show()
```