#! https://zhuanlan.zhihu.com/p/453466957
![Image](https://pic4.zhimg.com/80/v2-5db1a82996ec388725185ae900a58008.jpg)

# PySpark 字符串处理

```python
from pyspark.sql.types import *
from pyspark.sql import functions as F
```

## 1. 基本字符串处理

### 1.1 字符串格式拼接字符串
```python
df = spark.createDataFrame([(5, "hello")], ['a', 'b'])
df = df.withColumn('v', F.format_string('%d %s', df.a, df.b))
df.show()
```
```bash
>>> output Data:
>>>
+---+-----+-------+
|  a|    b|      v|
+---+-----+-------+
|  5|hello|5 hello|
+---+-----+-------+
```

### 1.2 字符串位置
```python
df.select(F.instr(df.v, 'h').alias('pos')).show()
```
```bash
>>> output Data:
>>>
+---+
|pos|
+---+
|  3|
+---+

```

### 1.3 字符串截取
```python
df.select(F.substring(df.v, 1, 5).alias('substring')).show()
```
```bash
>>> output Data:
>>>
+---------+
|substring|
+---------+
|    5 hel|
+---------+

```

## 2. 替换 `DataFrame` 中的列值

- `regexp_replace()`、`translate()`、 `overlay()`来替换 `PySpark DataFrame` 的列值。

### 2.1 创建一个带有一些地址的 `PySpark DataFrame`

使用这个 DataFrame 来解释如何替换列值。
```python
address = [
    (1,"14851 Jeffrey Rd","DE"),
    (2,"43421 Margarita St","NY"),
    (3,"13111 Siemon Ave","CA")]
df =spark.createDataFrame(
    address, ["id", "address", "state"])
df.show()
```
```bash
>>> output Data:
>>>
+---+------------------+-----+
| id|           address|state|
+---+------------------+-----+
|  1|  14851 Jeffrey Rd|   DE|
|  2|43421 Margarita St|   NY|
|  3|  13111 Siemon Ave|   CA|
+---+------------------+-----+

```

### 2.2 `regexp_replace()`替换字符串列值

使用正则表达式进行匹配，如果正则表达式不匹配则返回空字符串，
下面的示例将字段`address`名称`Rd`值替换为`Road`字符串。
```python
df.withColumn('address', F.regexp_replace('address', 'Rd', 'Road')).show()
```
```bash
>>> output Data:
>>>
+---+------------------+-----+
| id|           address|state|
+---+------------------+-----+
|  1|14851 Jeffrey Road|   DE|
|  2|43421 Margarita St|   NY|
|  3|  13111 Siemon Ave|   CA|
+---+------------------+-----+

```

### 2.3 有条件地替换列值

在上面的例子中，只是替换`Rd`，但没有替换`St`和`Ave`，
使用`when().otherwise()`条件函数有条件地替换列值。

```python
df.withColumn('address', 
    F.when(df.address.endswith('Rd'), F.regexp_replace(df.address,'Rd','Road'))\
   .when(df.address.endswith('St'), F.regexp_replace(df.address,'St','Street'))\
   .when(df.address.endswith('Ave'), F.regexp_replace(df.address,'Ave','Avenue'))\
   .otherwise(df.address)).show(truncate=False)
```
```bash
>>> output Data:
>>>
+---+----------------------+-----+
|id |address               |state|
+---+----------------------+-----+
|1  |14851 Jeffrey Road    |DE   |
|2  |43421 Margarita Street|NY   |
|3  |13111 Siemon Avenue   |CA   |
+---+----------------------+-----+

```

### 2.4 用字典替换列值

在下面的示例中，我们将`state`列的字符串值替换为字典键值对中的完整缩写名称，
为此我使用`PySpark map()`来循环遍历 `DataFrame` 的每一行。

#### 方法一：
```python
stateDic={'CA':'California','NY':'New York','DE':'Delaware'}
df2=df.rdd.map(lambda x: (x.id, x.address, stateDic[x.state]))\
    .toDF(["id", "address", "state"])
df2.show()
```
```bash
>>> output Data:
>>>
+---+------------------+----------+
| id|           address|     state|
+---+------------------+----------+
|  1|  14851 Jeffrey Rd|  Delaware|
|  2|43421 Margarita St|  New York|
|  3|  13111 Siemon Ave|California|
+---+------------------+----------+

```

#### 方法二：

- 参数：`subset` 指定要替换的列

```python
df.replace(stateDic, subset='state').show()
# 或者
df.replace(list(stateDic.keys()), list(stateDic.values()), subset='state').show()
```
```bash
>>> output Data:
>>>
+---+------------------+----------+
| id|           address|     state|
+---+------------------+----------+
|  1|  14851 Jeffrey Rd|  Delaware|
|  2|43421 Margarita St|  New York|
|  3|  13111 Siemon Ave|California|
+---+------------------+----------+

+---+------------------+----------+
| id|           address|     state|
+---+------------------+----------+
|  1|  14851 Jeffrey Rd|  Delaware|
|  2|43421 Margarita St|  New York|
|  3|  13111 Siemon Ave|California|
+---+------------------+----------+

```

### 2.5 逐个字符替换列值`translate()`

逐个字符地替换 `DataFrame` 列值。在下面的例子中，
每一个字符`1`替换为`A`，`2`替换为`B`，`3`替换为`C`。

```python
df.withColumn('address', F.translate('address', '123', 'ABC')).show()
```
```bash
>>> output Data:
>>>
+---+------------------+-----+
| id|           address|state|
+---+------------------+-----+
|  1|  A485A Jeffrey Rd|   DE|
|  2|4C4BA Margarita St|   NY|
|  3|  ACAAA Siemon Ave|   CA|
+---+------------------+-----+

```

### 2.6 用另一个列值替换列

通过使用`expr()` 和 `regexp_replace()`可以用另一个 `DataFrame column` 中的值替换列值。
```python
df = spark.createDataFrame(
   [("ABCDE_XYZ", "XYZ","FGH")], 
    ("col1", "col2","col3"))
df.withColumn(
    "new_column", F.expr("regexp_replace(col1, col2, col3)").alias("replaced_value")).show()
```
```bash
>>> output Data:
>>>
+---------+----+----+----------+
|     col1|col2|col3|new_column|
+---------+----+----+----------+
|ABCDE_XYZ| XYZ| FGH| ABCDE_FGH|
+---------+----+----+----------+

```

### 2.7 使用`overlay()`函数，用来自另一列的字符串值替换列值。

`overlay()`函数，用于从开始位置和字符数将字符串与另一个列字符串覆盖。

```python
df = spark.createDataFrame([("ABCDE_XYZ", "FGH")], ("col1", "col2"))
df.select('col1', 'col2', F.overlay("col1", "col2", 7).alias("overlayed")).show()
```
```bash
>>> output Data:
>>>
+---------+----+---------+
|     col1|col2|overlayed|
+---------+----+---------+
|ABCDE_XYZ| FGH|ABCDE_FGH|
+---------+----+---------+

```

## 3. 正则表达式

### 3.1 正则表达式提取
```python
df = spark.createDataFrame([('100-200', '中午01')], ['str', 'cn'])
df.select('str',
    F.regexp_extract('str', '(\d+)-(\d+)', 0).alias('pos-0'),
    F.regexp_extract('str', '(\d+)-(\d+)', 1).alias('pos-1'),
    F.regexp_extract('str', '(\d+)-(\d+)', 2).alias('pos-2'),
    F.regexp_extract(df.cn, "[\u4e00-\u9fa5]+", 0).alias('中文'),  # 提取全部的中文字符串
).show()
```
```bash
>>> output Data:
>>>
+-------+-------+-----+-----+----+
|    str|  pos-0|pos-1|pos-2|中文|
+-------+-------+-----+-----+----+
|100-200|100-200|  100|  200|中午|
+-------+-------+-----+-----+----+

```

### 3.2 字符串拼接
```python
df = spark.createDataFrame([('abcd','123')], ['str', 'int'])
df_new = df.select(
    F.concat(df.str, df.int).alias('concat'),    # 直接拼接
    F.concat_ws('-', df.str, df.int).alias('concat_ws'), # 指定拼接符
)
df_new.show()
```
```bash
>>> output Data:
>>>
+-------+---------+
| concat|concat_ws|
+-------+---------+
|abcd123| abcd-123|
+-------+---------+

```

### 3.3 字符串重复`repeat`
```python
# df 中的 str 列重复两次
df.select(F.repeat('str', 2).alias('str 重复两次')).show()
```
```bash
>>> output Data:
>>>
+------------+
|str 重复两次|
+------------+
|    abcdabcd|
+------------+

```

### 3.4 分割`split`

`F.split(str, pattern, limit=-1)`

- `limit <= 0`: 展示全部的拆分内容
- `limit > 0`: 不超过展示的最小条目数

```python
df_new.select(
    'concat_ws',
    F.split('concat_ws', '-', -1).alias('split array'), 
    F.split('concat_ws', '-', 0).alias('0'),
    F.split('concat_ws', '-', 1).alias('1'),
    F.split('concat_ws', '-', 2).alias('2'),
).show()
```
```bash
>>> output Data:
>>>
+---------+-----------+-----------+----------+-----------+
|concat_ws|split array|          0|         1|          2|
+---------+-----------+-----------+----------+-----------+
| abcd-123|[abcd, 123]|[abcd, 123]|[abcd-123]|[abcd, 123]|
+---------+-----------+-----------+----------+-----------+
```

### 3.5 `contains` 或 `rlike` 进行行选取
```python
# 展示包含 - 的数据
df_new.where(df_new.concat_ws.contains("-")).show()

# 展示包含 - 或 = 的数据
df_new.where(df_new.concat_ws.rlike("-|=")).show()
```
```bash
>>> output Data:
>>>
+-------+---------+
| concat|concat_ws|
+-------+---------+
|abcd123| abcd-123|
+-------+---------+

+-------+---------+
| concat|concat_ws|
+-------+---------+
|abcd123| abcd-123|
+-------+---------+

```

### 3.6 修改数据类型
```python
# 查看数据类型
df = spark.createDataFrame([('abcd', 123)], ['str', 'int'])
df.printSchema()
df.dtypes
```
```bash
>>> output Data:
>>>
root
 |-- str: string (nullable = true)
 |-- int: long (nullable = true)

```
```bash
>>> output Data:
>>> [('str', 'string'), ('int', 'bigint')]
```

#### 方式一：`withColumn` 覆盖原字段
```python
# cast
df.withColumn('int', df.int.cast(DoubleType())).printSchema()
```
```bash
>>> output Data:
>>>
root
 |-- str: string (nullable = true)
 |-- int: double (nullable = true)

```
```python
# astype
df.withColumn("int", df.int.astype(StringType())).printSchema()
```
```bash
>>> output Data:
>>>
root
 |-- str: string (nullable = true)
 |-- int: string (nullable = true)
```

#### 方式二： Select
```python
# cast
df.select(F.col('int').cast(StringType())).printSchema()

# astype
df.select(F.col('int').astype(StringType())).printSchema()
```
```bash
>>> output Data:
>>>
root
 |-- int: string (nullable = true)

root
 |-- int: string (nullable = true)
```

---
