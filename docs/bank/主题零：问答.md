# 小蛤蟆的问答

## 1. 偏差（Bias）与方差（Variable）/ 过拟合（Overfitting）与欠拟合（Underfitting）

- $Bias$和$Variance$是针对模型$Generalization$（泛化）来说的；$Generalization\ error$主要可细分为$Bias$和$Variance$两个部分。

$$Error = Bias + Variance + Noise$$

- $Error$: 反映的是整个模型的准确度，
- $Bias$: 反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度， 
- $Variance$: 反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。
- $Noise$: 反映的是模型与数据不可避免的噪声。

<center>
<img src='./img/bias-and-variance_orig.png' width="480px">
</center>

<center>上图：模型的偏差与方差</center>

<center>
<img src='./img/overfit-ubderfit_orig.png' width="480px">
</center>

上图可以观察到：
1. 在 $Underfitting$ 的时候，不论是在 $Training\ set$ 还是 $Validation set$ 的 $Error$ 都很高。
2. 在 $Overfitting$ 的时候，$Training\ set$ 的 $Error$ 已经将降低了，但 $Validation\ set$ 上的 $Error$ 会很高。

**过拟合解决方法**
1. $Early\ Stopping$
2. 增加训练资料
3. 降低特征维度
4. 如果没有使用正规化($Regularization$)可以将其加入
5. 调整超参数(修改模型架构)
6. 更换一个较为简单的模型
7. 神经网络增加dropout层

**欠拟合解决方法**
1. 增加训练的叠代次数
2. 调整超参数( 修改模型架构 )
3. 生成更多的特征来训练模型
4. 如果有使用正规化($Regularization$)可先将其移除
5. 更换一个更复杂的模型

实际上，在建模过程中一般观测模型的偏差与方差来判断模型是否发生过拟合、欠拟合。进而采取一定的措施降低偏差与方差，避免欠拟合、过拟合。

## 2. 逐步回归（Stepwise Regression）

**经过逐步回归，使得最后保留在模型中的解释变量既是重要的，又没有严重多重共线性。**

> 方式一：$Forward\ selection$

将变量逐个引入模型，每引入一个解释变量后都要进行F检验，并对已经选入的解释变量逐个进行t检验，
当原来引入的解释变量由于后面解释变量的引入变得不再显著时，停止变量选择。



> 方式二：$Backward\ elimination$


> 方式三：$Bidirectional\ elimination$



## 3. 多重共线性

> 多重共线性成因

多重共线性问题就是指一个解释变量的变化引起另一个解释变量地变化。

原本自变量应该是各自独立的，根据回归分析结果，能得知哪些因素对因变量Y有显著影响，哪些没有影响。如果各个自变量x之间有很强的线性关系，就无法固定其他变量，也就找不到x和y之间真实的关系了。

除此以外，多重共线性的原因还可能包括：

- 数据不足。在某些情况下，收集更多数据可以解决共线性问题。
- 错误地使用虚拟变量。（比如，同时将男、女两个虚拟变量都放入模型，此时必定出现共线性，称为完全共线性）

> 多重共线性检测手段

有多种方法可以检测多重共线性，较常使用的是回归分析中的VIF值，VIF值越大，多重共线性越严重。一般认为VIF大于10时（严格是5），代表模型存在严重的共线性问题。

有时候也会以容差值作为标准，容差值=1/VIF，所以容差值大于0.1则说明没有共线性(严格是大于0.2)，VIF和容差值有逻辑对应关系，两个指标任选其一即可。

除此之外，直接对自变量进行相关分析，查看相关系数和显著性也是一种判断方法。如果一个自变量和其他自变量之间的相关系数显著，则代表可能存在多重共线性问题

> 多重共线性解决手段

多重共线性是普遍存在的，通常情况下，如果共线性情况不严重（VIF<5），不需要做特别的处理。如存在严重的多重共线性问题，可以考虑使用以下几种方法处理：

**1. 手动移除出共线性的变量**

先做下[相关分析，如果发现某两个自变量X（解释变量）的相关系数值大于0.7，则移除掉一个自变量（解释变量），然后再做回归分析。此方法是最直接的方法，但有的时候我们不希望把某个自变量从模型中剔除，这样就要考虑使用其他方法。

**2. 逐步回归法**

让系统自动进行自变量的选择剔除，使用逐步回归将共线性的自变量自动剔除出去。此种解决办法有个问题是，可能算法会剔除掉本不想剔除的自变量，如果有此类情况产生，此时最好是使用岭回归进行分析。

**3. 增加样本容量**

增加样本容量是解释共线性问题的一种办法，但在实际操作中可能并不太适合，原因是样本量的收集需要成本时间等。

**4. 岭回归**

上述第1和第2种解决办法在实际研究中使用较多，但问题在于，如果实际研究中并不想剔除掉某些自变量，某些自变量很重要，不能剔除。此时可能只有岭回归最为适合了。岭回归是当前解决共线性问题最有效的解释办法。

> 其他问题

1. ##### 多重共线性与相关性的区别

相关性，是指两个变量的关联程度。一般地，从散点图上可以观察到两个变量有以下三种关系之一：两变量**正相关、负相关、不相关**。如果一个变量高的值对应于另一个变量高的值，相似地，低的值对应低的值，那么这两个变量正相关。

1. ##### 多重共线性不需要处理的场景

1.多重共线性是普遍存在的，轻微的多重共线性问题可不采取措施，如果VIF值大于10说明共线性很严重，这种情况需要处理，如果VIF值在5以下不需要处理，如果VIF介于5~10之间视情况而定。

2.如果模型仅用于预测，则只要拟合程度好，可不处理多重共线性问题，存在多重共线性的模型用于预测时，往往不影响预测结果。

## 4. 逻辑回归

> 逻辑函数

逻辑函数（Logistic Function）又称为 Sigmoid 函数：

​                                                                                  $g(z) = \frac{1}{1+e^{-z}}$​   

它的特性是所有值都在 (0,1) 之间，如图所示。

![Sigmoid函数](C:\Users\Chensy\OneDrive\代码及文档\MyDocsify\docs\img\sigmoid.gif) 

逻辑函数的作用是判断不同属性的样本属于某个类别的概率。在二分类过程中，用1表示正向的类别，用0表示负向的类别，也就是说经过Sigmoid函数转换，如果值越靠近1则其属于正向类别的概率越大；如果值越靠近0，则其属于负向类别的概率越大。

逻辑回归可以看做是两步，第一步和线性回归模型的形式相同，即一个关于输入x的线性函数：

​															$z = W^T X+ b$​

第二步通过一个逻辑函数，即sigmoid函数，将线性函数转换为非线性函数。

​                                                      $z = W^T X+b , \sigma(z) = \frac{1}{1+e^{-z}}$

逻辑回归就是将逻辑函数用在线性回归函数上层，将回归问题转换成分类问题。

> Loss Function



> 优化手段

- 极大似然
- 最小二乘
- 梯度下降

## 5. 线性回归

> 线性回归的基本假设

1. 随机误差项是一个期望值或平均值为0的随机变量；
2. 对于解释变量的所有观测值，随机误差项有相同的方差；
3. 随机误差项彼此不相关；
4. 解释变量是确定性变量，不是随机变量，与随机误差项彼此之间相互独立；
5. 解释变量之间不存在精确的（完全的）线性关系，即解释变量的样本观测值矩阵是满秩矩阵；
6. 随机误差项服从正态分布。

(以上基本假设的数学表达是什么？)

> 违背线性回归基本假设时如何检测
- DW检验（杜宾瓦尔森检验）

> 违背线性回归基本假设时如何修正

> Loss Function

> 优化手段

> 拟合优度 R-squared

> 调整拟合优度 Adj. R-squared

> T检验

> F检验

> AIC

> BIC



- OLS

## 6. Softmax Mode

> Softmax Function

> Loss Function

## 7. 特征衍生

1. 衍生考虑角度（维度）有哪些？

## 8. 特征工程

1. 什么是特征工程？
2. 为什么要做特征工程？
3. 如何做特征工程？

> 

## 9. 特征选择

> 特征选择的方法论

1. 选择业务含义明确的变量
2. ...(补充)

> 基本的特征选择手段、评估方法

1. 单变量回归分析
2. ...(补充)

> 

## 10. 模型评估手段

> 

## 11. 建模流程

> 数据分析

数据分析（描述性统计分析）阶段需要考虑哪些问题？

1. 缺失值、极值、峰度、偏度、众数、中位数、频繁数；

**离群（异常）值分析**

- [异常值检测](https://zhuanlan.zhihu.com/p/72404640)

> $3\sigma$法则

如果一个数据分布近似正态：
- 大约 $68%$ 的数据值会在均值的一个标准差范围内
- 大约 $95%$ 会在两个标准差范围内
- 大约 $99.7%$ 会在三个标准差范围内

<center>
<img src="./img/3sigma.jpg" width="480px">
</center>

如果数据点超过标准差的 $3$ 倍，那么这些点很有可能是异常值或离群点。

> 分位数

<center>
<img src="./img/box_outlier.jpg" width="480px">
</center>

四分位间距 ($IQR$) 的概念被用于构建箱形图。$IQR$ 是统计学中的一个概念，
通过将数据集分成四分位来衡量统计分散度和数据可变性。

$$
IQR = Q3 -Q1
$$

离群点被定义为：
1. 低于箱形图下触须（$Q1 − 1.5 \times IQR$）
2. 高于箱形图上触须（$Q3 + 1.5 \times IQR$）

<center>
<img src="./img/3sigma-box.jpg" width="480px">
</center>

## 12. 

---

**以上仅供参考**

---

**参考**
1. [偏差与方差之权衡Bias-Variance Tradeoff](https://jason-chen-1992.weebly.com/home/-bias-variance-tradeoff)

