# 决策树

## 信息熵

> 信息熵

$$
H(X) = - \sum_{x \in X} p(x) log p(x)
$$

信息熵的三条性质：

1. 单调性，即发生概率越高的事件，其所携带的信息熵越低。极端案例就是“太阳从东方升起”，因为为确定事件，所以不携带任何信息量。
从信息论的角度，认为这句话没有消除任何不确定性。
2. 非负性，即信息熵不能为负。这个很好理解，因为负的信息，即你得知了某个信息后，却增加了不确定性是不合逻辑的。
3. 累加性，即多随机事件同时发生存在的总不确定性的量度是可以表示为各事件不确定性的量度的和。

> 数据集信息熵

$$
H(D) = - \sum_{k=1}^{K} \frac{|C_k|}{|D|} log_2 \frac{|C_k|}{|D|}
$$

其中$C_k$表示集合$D$中属于第$k$类样本的样本子集。

> 条件熵

$$
H(D|A) = \sum_{A_i = 1}^{A} \frac{D_{A_i}}{}
$$

## 基尼系数

> Gini

1. 是一种不等性度量；通常用来度量收入不平衡（0.4为收入分配警戒线），可以用来度量任何不均匀分布；
2. 是介于0~1之间的数，0-完全相等，1-完全不相等；
3. 总体内包含的类别越杂乱，GINI指数就越大，跟熵的概念很相似。

<center>
<img src="./img/gini.png" width="280px">
</center>
<center>上图曲线为洛伦兹曲线，$X$轴为人口百分比，$Y$轴为该分口百分比下收入占比。</center>

$A$、$B$ 为图形面积，则基尼系数$G$定义为：

$$
G = \frac{A}{A+B}
$$

> 示例代码

```python
import numpy as np
from matplotlib import pyplot as plt

def GiniIndex(vals, plot=None):
    '''基尼系数'''
    vals = np.array(vals)
    cum = np.cumsum(sorted(np.append(vals, 0)))
    max_val = cum[-1]
    x = np.arange(len(cum)) / len(vals)
    y = cum / max_val
    B = np.trapz(y, x=x)
    A = 0.5 - B
    G = A / (A + B)
    if plot:
        fig, ax = plt.subplots()
        plt.xticks([0, 1.0])
        plt.yticks([1.0])
        plt.axis('scaled')
        ax.plot(x, y, color='black')
        ax.plot(x, x, color='black')
        ax.plot([0, 1, 1, 1], [0, 0, 0, 1], color='black')
        ax.fill_between(x, y)
        ax.fill_between(x, x, y, where=y <= x)
        plt.show()
    return G
```

**补充`np.trapz`**

- 使用复合梯形规则沿给定轴积分。 沿给定轴对$y(x)$进行积分。
- [更多参考](https://blog.csdn.net/weixin_44338705/article/details/89203791)。

> 基尼不纯度指标

基尼不纯度表示一个随机选中的样本在子集中被分错的可能性（在$CART$算法中就用了此方法评估指标）。
基尼不纯度为这个样本被选中的概率乘以它被分错的概率。当一个节点中所有样本都是一个类时，
基尼不纯度为零。

对信息熵（$H(X) = - \sum_{x \in X} p(x) log p(x)$）中$-log(p(x))$进行泰勒展开后，忽略高阶项（因为p小于1，所以高阶近似于0），
就得到了基尼不纯度的公式:

$$
Gini(X) = \sum_{x \in X}p(x)log(1-p(x))
$$

**$ln(x)$泰勒级数展开：**

$$
ln (x) = (x - 1) - \frac{(x-1)^2}{2} + \frac{(x-1)^3}{3} - ... (0 < x \le 2)
$$

[常用泰勒级数](http://www.ab126.com/shuxue/3821.html)

从基尼不纯度的公式上看是熵的一个近似值。可以衡量不确定性的大小，衡量杂乱程度也就是不纯度。

<center>
<img src="./img/gini_entropy.png" width="380px">
</center>
p为0或1时，熵、基尼不纯度、和分类误差率都为0，代表不存在不确定性；
当p为0.5时，也就是两个类别的概率相等时，熵、基尼不纯度、和分类误差率最大，不确定性最大。

> 
> 