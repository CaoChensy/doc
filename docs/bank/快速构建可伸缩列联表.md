#! https://zhuanlan.zhihu.com/p/448856846

![Image](https://pic4.zhimg.com/80/v2-e8397431bf2a9e53ff112fba6fcc7fc4.png)

# 快速构建可伸缩列联表

在大数据统计实践中，通常我们面对的是亿级或亿级以上的数据量级，在对如此量级的数据进行统计的时候。

一般数据分析人员常用`pandas.DataFrame`进行快速的数据分析，

但是在大数据的情况下，使用`pyspark.DataFrame.toPandas`的方式对数据转成`Pandas`进行指标的统计，

由于内存限制这是是完全不现实的。而仅使用`pyspark.DataFrame`进行统计，在数据量较大时运算速度也相对较慢。

难以满足即时的数据分析。


本文采取一种方式解决以上问题。

## 创建随机数据
```python
import numpy as np
import pandas as pd
from pyspark.sql import functions as F
```

> 创建一个50万数据量级的随机数据DataFrame，随机数据中的字段如下：

- `id`: 企业ID
- `ent`: 企业一种状态
- `scope`: 企业规模
- `date`: 时间日期
- `prov`: 企业所属省份
```python
#### 样本量
size = int(1e5)

ent = [0, 1]
scope = ['0大', '1中', '2小', '3微']
dates = pd.date_range(start='20190101', end='20200101', freq='M')\
    .astype(str).str.replace('-', '').tolist()
prov = [
    '浙江省', '江苏省', '上海市', '山东省', 
    '安徽省', '海南省', '北京市', '江西省']
```
```python
df = pd.DataFrame({
    'id': np.arange(size),
    'ent': [ent[i] for i in np.random.binomial(1, 0.9, size)],
    'scope': [scope[i] for i in np.random.randint(0, len(scope), size=size)],
    'prov': [prov[i] for i in np.random.randint(0, len(prov), size=size)],
    'data': np.random.randint(0, 1e6, size),
    'dates': [dates[i] for i in np.random.randint(0, len(dates), size=size)],
})
df = spark.createDataFrame(df)
#### 展示数据
df.show(5)
```
```bash
>>> output Data:
>>>
+---+---+-----+------+------+--------+
| id|ent|scope|  prov|  data|   dates|
+---+---+-----+------+------+--------+
|  0|  1|  1中|上海市| 80903|20190228|
|  1|  1|  3微|安徽省|866458|20190531|
|  2|  1|  0大|上海市| 16959|20190630|
|  3|  1|  0大|山东省|371351|20190228|
|  4|  1|  2小|江苏省|994056|20190731|
+---+---+-----+------+------+--------+
only showing top 5 rows

```

## `GroupBy` 分组统计

在`PySpark`中对大数据中的类别字段进行分组统计。
```python
df_stats = df.groupBy('ent', 'scope', 'prov', 'dates').agg(
    F.countDistinct('id').alias('id'),
    F.sum('data').alias('sum_data'),
    F.avg('data').alias('avg_data'),
).toPandas()
```
```python
#### 2（状态） * 4（规模） * 12（月份） * 8（省份） = 768
df_stats.shape
```
```bash
>>> output Data:
>>> (768, 7)
```

以上可以看到，对类别字段分组统计后，数据量级从50万，下降至768，数据量级大大减少，

可以方便的转到`Pandas.DataFrame`进行下一步的统计。

## 统计（全量）各省各月份企业占比

- `index: date`

- `column: 省份`
```python
#### 使用 pivot 制作列联表
df_date_prov = pd.pivot_table(df_stats, index='dates', columns='prov', values='id', aggfunc='sum')
#### 统计占比
df_date_prov_rate = df_date_prov.div(df_date_prov.sum(axis=1), axis=0)

#### 企业数
display(df_date_prov)
#### 占比
display(df_date_prov_rate.round(3))
```
```bash
>>> output Data:
>>> prov       上海市   北京市   安徽省   山东省   江苏省   江西省   浙江省   海南省
>>> dates                                                   
>>> 20190131  1071  1102  1016  1073   993  1025   991  1074
>>> 20190228  1111  1009  1090  1026  1051  1012  1067  1048
>>> 20190331  1027  1077  1042  1004   935  1067  1018  1058
>>> 20190430  1074  1040  1080   987  1035  1067  1011  1044
>>> 20190531  1088  1025  1063  1029  1014  1080  1043  1078
>>> 20190630  1072  1049  1011  1017  1026   987  1071  1024
>>> 20190731  1108  1068  1107  1032  1065  1026   988   986
>>> 20190831  1105  1020  1030  1082  1078  1041  1028   992
>>> 20190930   999  1055  1065   998  1021   967  1063  1034
>>> 20191031  1034  1033  1038  1004  1027  1033  1052  1018
>>> 20191130  1053  1038  1029  1027  1046  1068  1068   996
>>> 20191231  1020  1078  1122  1069  1044  1082   997  1064
```
```bash
>>> output Data:
>>> prov        上海市    北京市    安徽省    山东省    江苏省    江西省    浙江省    海南省
>>> dates                                                           
>>> 20190131  0.128  0.132  0.122  0.129  0.119  0.123  0.119  0.129
>>> 20190228  0.132  0.120  0.130  0.122  0.125  0.120  0.127  0.125
>>> 20190331  0.125  0.131  0.127  0.122  0.114  0.130  0.124  0.129
>>> 20190430  0.129  0.125  0.130  0.118  0.124  0.128  0.121  0.125
>>> 20190531  0.129  0.122  0.126  0.122  0.120  0.128  0.124  0.128
>>> 20190630  0.130  0.127  0.122  0.123  0.124  0.120  0.130  0.124
>>> 20190731  0.132  0.127  0.132  0.123  0.127  0.122  0.118  0.118
>>> 20190831  0.132  0.122  0.123  0.129  0.129  0.124  0.123  0.118
>>> 20190930  0.122  0.129  0.130  0.122  0.124  0.118  0.130  0.126
>>> 20191031  0.126  0.125  0.126  0.122  0.125  0.125  0.128  0.124
>>> 20191130  0.126  0.125  0.124  0.123  0.126  0.128  0.128  0.120
>>> 20191231  0.120  0.127  0.132  0.126  0.123  0.128  0.118  0.126
```

## 统计（小微）各省各月份企业占比
```python
df_stats_micro = df_stats[df_stats.scope.isin(['2小', '3微'])]
df_date_prov = pd.pivot_table(
    df_stats_micro, index='dates', columns='prov', values='id', aggfunc='sum')
df_date_prov_rate = df_date_prov.div(df_date_prov.sum(axis=1), axis=0)

display(df_date_prov)
display(df_date_prov_rate.round(3))
```
```bash
>>> output Data:
>>> prov      上海市  北京市  安徽省  山东省  江苏省  江西省  浙江省  海南省
>>> dates                                           
>>> 20190131  548  545  514  516  490  529  500  539
>>> 20190228  556  497  549  520  523  475  555  530
>>> 20190331  535  538  519  487  479  549  489  542
>>> 20190430  537  538  525  498  532  526  514  547
>>> 20190531  558  536  503  539  507  538  504  535
>>> 20190630  544  540  459  513  504  491  529  536
>>> 20190731  539  551  561  498  501  518  507  514
>>> 20190831  584  514  482  543  555  519  516  471
>>> 20190930  506  531  537  470  481  496  519  492
>>> 20191031  551  499  531  523  511  517  520  499
>>> 20191130  550  541  539  487  504  543  543  518
>>> 20191231  506  554  559  526  513  526  481  521
```
```bash
>>> output Data:
>>> prov        上海市    北京市    安徽省    山东省    江苏省    江西省    浙江省    海南省
>>> dates                                                           
>>> 20190131  0.131  0.130  0.123  0.123  0.117  0.127  0.120  0.129
>>> 20190228  0.132  0.118  0.131  0.124  0.124  0.113  0.132  0.126
>>> 20190331  0.129  0.130  0.125  0.118  0.116  0.133  0.118  0.131
>>> 20190430  0.127  0.128  0.124  0.118  0.126  0.125  0.122  0.130
>>> 20190531  0.132  0.127  0.119  0.128  0.120  0.127  0.119  0.127
>>> 20190630  0.132  0.131  0.112  0.125  0.122  0.119  0.129  0.130
>>> 20190731  0.129  0.132  0.134  0.119  0.120  0.124  0.121  0.123
>>> 20190831  0.140  0.123  0.115  0.130  0.133  0.124  0.123  0.113
>>> 20190930  0.125  0.132  0.133  0.117  0.119  0.123  0.129  0.122
>>> 20191031  0.133  0.120  0.128  0.126  0.123  0.125  0.125  0.120
>>> 20191130  0.130  0.128  0.128  0.115  0.119  0.129  0.129  0.123
>>> 20191231  0.121  0.132  0.134  0.126  0.123  0.126  0.115  0.124
```

----
